{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. [20 pts] List three popular RNN types, then briefly compare and contrast their features and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three popular RNN types are Elman RNNs, LSTM RNNs, and GRU RNNs. Elman RNNs are simpler RNNs with a hidden layer that captures information about the previous time step and feeds it back into the network for the current time step. The LSTM is another RNN that tries to fix the vanishing gradient problem, where input, output, and forget gate are added to the architecture. The GRU is a variant of the LSTM, where the LSTM gates are substituted for update and reset gates. \n",
    "\n",
    "Some benefits of the GRU over LSTM include a simpler architecture, which saves memory. Elman is also simpler than GRU and LSTM, although it suffers far more from the vanishing gradient problem than the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. [20 pts] Explain why the LSTM RNN performed superbly compared to the remainder of the classifiers in the module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM RNN performed a lot more efficiently than the remainder of the classifiers for a few reasons. First, One of the main reasons that the LSTM RNN performed a lot better is due to the ability of the LSTM to mitigate the vanishing gradient problem. Vanishing gradient problem is a phenomenon that occurs during the training of deep neural networks, where the gradients that are used to update the network become extremely small or \"vanish\" as they are backpropogated from the output layers to the earlier layers. Due to this, as well as other mechanisms to improve efficiency,this makes them perform far better than a regular RNN does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. [20 pts] From the surname dataset, using only two languages {English and Scottish}, build a classifier to distinguish between them. Report 10-fold cross validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, path\n",
    "import numpy as np\n",
    "PATH_DATA='./surnames'\n",
    "LANGS=('English','Scottish')\n",
    "SEQ_SIZE=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letter index 0 is the padding value, i.e. padding to fill up the vector to SEQ_SIZE, necessary for batched\n",
    "# Note that eventually we will use torch Tensor to represent these fixed length sequences\n",
    "LetterVocabulary, LetterVocabularyIndex, Index2Voc, Sequences = {' ':0}, 1, {0:' '}, {}\n",
    "\n",
    "for fn in sorted([_ for _ in listdir(PATH_DATA) if _.endswith('.txt')]):\n",
    "    lang, seqs = path.splitext(path.basename(fn))[0], []\n",
    "\n",
    "    if lang not in LANGS:  # test case\n",
    "        continue\n",
    "\n",
    "    with open(path.join(PATH_DATA, fn), 'r', encoding=\"utf8\") as fin:\n",
    "        for row in fin.read().splitlines():\n",
    "            seq = np.zeros(SEQ_SIZE, dtype=np.int32)\n",
    "            for i, letter in enumerate(row.lower()):  # Convert the surname to lower case\n",
    "#            for i, letter in enumerate(row):\n",
    "                if i < SEQ_SIZE:\n",
    "                    if letter not in LetterVocabulary:\n",
    "                        LetterVocabulary[letter] = LetterVocabularyIndex\n",
    "                        Index2Voc[LetterVocabularyIndex] = letter\n",
    "                        LetterVocabularyIndex += 1\n",
    "                    seq[i] = LetterVocabulary[letter]\n",
    "            seqs += [seq]\n",
    "    Sequences[lang] = seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abbey          [1 2 2 4 5 0 0 0 0 0 0 0 0 0]\n",
      "wilson         [18  9 10  3  6 16  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "def print_names(_lang, _k):\n",
    "    print(''.join([Index2Voc[c] for c in Sequences[_lang][_k]]), Sequences[_lang][_k]) if _lang in LANGS else None\n",
    "\n",
    "# Some examples\n",
    "print_names('English', 1)\n",
    "print_names('Scottish', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3768"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Sequences['English'])+len(Sequences['Scottish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3768 14 2\n"
     ]
    }
   ],
   "source": [
    "# Sanity\n",
    "N = sum([len(Sequences[_]) for _ in Sequences])\n",
    "T = Sequences['English'][0].shape[0]\n",
    "C = len(np.unique(Sequences.keys())[0])\n",
    "print(N, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M= 26\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Pool all sequences and all languages\n",
    "Seqs = [Sequences[LANGS[_]] for _ in range(C)]\n",
    "Seqs = list(itertools.chain(*Seqs))\n",
    "\n",
    "# Number of features is the number of unique characters\n",
    "M = np.max(Seqs)\n",
    "print(f'M= {M}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0265, 0.9735])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Apriori class balance, i.e. inverse probability of the class\n",
    "nk = np.array([len(Sequences[LANGS[_]]) for _ in range(C)], dtype=np.float32)\n",
    "nk = (N/nk)\n",
    "nk = nk/nk.sum()\n",
    "\n",
    "# Class weights, inverse apriori probability\n",
    "WEIGHTS = torch.tensor(nk, dtype=torch.float32)\n",
    "\n",
    "print(WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "y = [[_]*len(Sequences[LANGS[_]]) for _ in range(C)]\n",
    "y = np.array(list(itertools.chain(*y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.empty((N,M))\n",
    "n = 0\n",
    "for lang in Sequences.keys():\n",
    "    for seq in Sequences[lang]:\n",
    "        sxx = np.zeros((M,), dtype=np.float32)\n",
    "        for i in range(SEQ_SIZE): # for the duration of the signal\n",
    "            if seq[i] > 0:\n",
    "                sxx[seq[i]-1] = 1\n",
    "        X[n] = sxx\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 426 ms, sys: 368 ms, total: 794 ms\n",
      "Wall time: 3.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Borrowed from previous lectures\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def kfold_eval_docs(_clf, _X, _y):\n",
    "    # Need indexable data structure\n",
    "    acc = []\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=False, random_state=None)\n",
    "    for train_index, test_index in kf.split(_X, _y):\n",
    "        _clf.fit(_X[train_index], _y[train_index])\n",
    "        y_pred = _clf.predict(_X[test_index])\n",
    "        acc += [accuracy_score(_y[test_index], y_pred)]\n",
    "    return np.array(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3768\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode every position of the sequence\n",
    "# List of sequence, language tuples for easy shuffling\n",
    "LANGS_CAT = dict(zip(LANGS, range(len(LANGS))))\n",
    "\n",
    "def get_Xy():\n",
    "    Xy = []\n",
    "    for lang in Sequences.keys():\n",
    "        for seq in Sequences[lang]:\n",
    "            T = SEQ_SIZE  # necessary for batched\n",
    "            sxx = np.zeros((T, M))\n",
    "            for i in range(T):  # for the duration of the signal\n",
    "                if seq[i] > 0:\n",
    "                    sxx[i, seq[i]-1] = 1\n",
    "            Xy += [(torch.tensor(sxx, dtype=torch.float32),\n",
    "                    torch.tensor([LANGS_CAT[lang]], dtype=torch.int64))]\n",
    "    return Xy\n",
    "\n",
    "# Helper functions\n",
    "def get_X(_Xy):\n",
    "    return [_[0] for _ in _Xy]\n",
    "    \n",
    "def get_y(_Xy):\n",
    "    return [int(_[1].data[0]) for _ in _Xy]\n",
    "\n",
    "# Sanity\n",
    "Xy = get_Xy()\n",
    "print(len(Xy))\n",
    "\n",
    "# printing the confusion matrix below\n",
    "def get_cm(_y, _p):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import pandas as pd\n",
    "\n",
    "    cm = confusion_matrix(_y, _p, labels=list(range(len(LANGS))))\n",
    "    display(pd.DataFrame(cm, index=[_[:5] for _ in LANGS], columns=[_[:5] for _ in LANGS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the GPU to device 0\n",
    "import torch.nn as nn\n",
    "gpu = torch.device('cpu')\n",
    "\n",
    "class My_RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden, n_hid_layers=1, epochs=10, eta=0.0005, batch_size=100, weight=None, info=True):\n",
    "        \"\"\" A PyTorch neural network model based on RNN cell, batched \"\"\"\n",
    "        super(My_RNN, self).__init__()\n",
    "\n",
    "        self.n_hidden= n_hidden  # hidden layer size\n",
    "        self.n_hid_layers= n_hid_layers  # number of hidden layers\n",
    "        self.epochs= epochs  # number of learning iterations\n",
    "        self.eta= eta  # learning rate\n",
    "        self.B= batch_size  # size of training batch - 1 would not work\n",
    "        self.info= info  # debug info\n",
    "        \n",
    "        self.rnn, self.outlayer = None, None\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        # loss function, since the last layer is nn.LogSoftmax\n",
    "        self.criterion = nn.NLLLoss(weight=weight)\n",
    "\n",
    "    def forward(self, _X, _h0):\n",
    "        output, hn = self.rnn(_X, _h0)\n",
    "        output = self.outlayer(output[:, -1, :])  # output is batched\n",
    "        output = self.softmax(output)\n",
    "        return output, hn\n",
    "    \n",
    "    def init_cell(self, _M):  # Create variations of our RNN by overriding init_cell\n",
    "        dropout = 0.2 if self.n_hid_layers > 1 else 0\n",
    "        return nn.RNN(_M, self.n_hidden, self.n_hid_layers,\n",
    "                      nonlinearity='relu',\n",
    "                      bias=False, batch_first=True, dropout=dropout)\n",
    "\n",
    "    def init_hidden(self, _B):  # batch_first = True\n",
    "        return torch.zeros(self.n_hid_layers, _B, self.n_hidden).to(gpu)  # Extra dimension - batch\n",
    "\n",
    "    def fit(self, _Xy):\n",
    "        from random import shuffle\n",
    "        import sys\n",
    "        import torch.optim as optim\n",
    "\n",
    "        M= _Xy[0][0].shape[1]  # number of features, based on batch input\n",
    "        C= np.unique([int(_[1].data[0]) for _ in _Xy]).shape[0]  # number of class labels\n",
    "\n",
    "        self.rnn = self.init_cell(M).to(gpu)\n",
    "        self.outlayer = nn.Linear(self.n_hidden, C).to(gpu)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.eta)\n",
    "        \n",
    "        for e in range(self.epochs):\n",
    "            # Shuffle the input to randomly interleave classes, note that they are tuples, i.e. (x, y)\n",
    "            shuffle(_Xy)\n",
    "\n",
    "            N = len(_Xy)\n",
    "            L, totloss = 0, 0\n",
    "\n",
    "            while L < N-self.B:\n",
    "                sxx = torch.stack([_[0] for _ in _Xy[L:L+self.B]]).to(gpu)\n",
    "                y = torch.tensor([_[1] for _ in _Xy[L:L+self.B]], dtype=torch.int64).to(gpu)\n",
    "                output, loss = self.train_signal(sxx, y, self.B)\n",
    "                \n",
    "                totloss += loss\n",
    "                L += self.B\n",
    "                \n",
    "                if self.info:\n",
    "                    sys.stderr.write(f\"\\r{e+1:03d}/{self.epochs:4d} | Loss: {loss:6.2f} | \"\n",
    "                                     f\"Avg loss: {totloss/(e+1):6.2f} | {y.data.tolist()[0]}\")\n",
    "                    sys.stderr.flush()\n",
    "    \n",
    "    def train_signal(self, _sxx, _y, _B):\n",
    "        h0 = self.init_hidden(_B)\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output, hn = self.forward(_sxx, h0)\n",
    "\n",
    "        loss = self.criterion(output, _y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return output, loss.item()\n",
    "\n",
    "    def predict(self, _sxx):  # Tensor dimensions: B x T x M\n",
    "        _sxx = torch.stack(_sxx)\n",
    "        with torch.no_grad():\n",
    "            h0 = self.init_hidden(_sxx.shape[0])  # reset the hidden layer\n",
    "            output, hn = self.forward(_sxx.to(gpu), h0)\n",
    "\n",
    "        p_values, indices = output.max(dim=1)\n",
    "        return indices.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.69 | Avg loss:   0.35 | 0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.70 | Avg loss:   0.35 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.69 | Avg loss:   0.35 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "001/1000 | Loss:   0.69 | Avg loss:   0.69 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "001/1000 | Loss:   0.69 | Avg loss:   0.69 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.70 | Avg loss:   0.35 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.69 | Avg loss:   0.35 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.69 | Avg loss:   0.35 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.69 | Avg loss:   0.35 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.70 | Avg loss:   0.35 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000/1000 | Loss:   0.05 | Avg loss:   0.00 | 0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN 10-fold CV Acc= 0.67 ±0.164\n",
      "CPU times: user 27min 30s, sys: 22.4 s, total: 27min 53s\n",
      "Wall time: 13min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Xy = get_Xy()\n",
    "\n",
    "cm_y, cm_p = [], []\n",
    "\n",
    "Acc = []\n",
    "kf = StratifiedKFold(n_splits=10)\n",
    "i=0\n",
    "for tr_ix, ts_ix in kf.split(np.arange(len(Xy)), get_y(Xy)):\n",
    "    i+=1\n",
    "    print(i)\n",
    "    rnn = My_RNN(128, n_hid_layers=2, epochs=1000, eta=0.005, batch_size=2000, weight=WEIGHTS, info=True).to(gpu)\n",
    "    \n",
    "    X_tr = [Xy[_] for _ in tr_ix]  # predict uses X and y as a tuple\n",
    "    X_ts = get_X([Xy[_] for _ in ts_ix])\n",
    "    y_ts = get_y([Xy[_] for _ in ts_ix])\n",
    "    \n",
    "    rnn.fit(X_tr)\n",
    "    y_pred = rnn.predict(X_ts)\n",
    "    \n",
    "    Acc += [np.sum(np.array(y_pred) == np.array(y_ts))/len(y_pred)]\n",
    "\n",
    "    cm_y += y_ts\n",
    "    cm_p += y_pred.tolist()\n",
    "\n",
    "print(f'RNN 10-fold CV Acc= {np.mean(Acc):.2f} {chr(177)}{np.std(Acc):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. [20 pts] Set the WEIGHTS to None and report the classification performance of (3.). Explain what caused the performance drop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.66 | Avg loss:   0.33 | 0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.63 | Avg loss:   0.32 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.63 | Avg loss:   0.32 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.75 | Avg loss:   0.37 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.73 | Avg loss:   0.36 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "001/1000 | Loss:   0.68 | Avg loss:   0.68 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.62 | Avg loss:   0.31 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.69 | Avg loss:   0.34 | 01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.67 | Avg loss:   0.33 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.66 | Avg loss:   0.33 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000/1000 | Loss:   0.03 | Avg loss:   0.00 | 0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN 10-fold CV Acc= 0.74 ±0.214\n",
      "CPU times: user 27min 39s, sys: 27.8 s, total: 28min 7s\n",
      "Wall time: 14min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Xy = get_Xy()\n",
    "\n",
    "cm_y, cm_p = [], []\n",
    "\n",
    "Acc = []\n",
    "kf = StratifiedKFold(n_splits=10)\n",
    "i=0\n",
    "for tr_ix, ts_ix in kf.split(np.arange(len(Xy)), get_y(Xy)):\n",
    "    i+=1\n",
    "    print(i)\n",
    "    rnn = My_RNN(128, n_hid_layers=2, epochs=1000, eta=0.005, batch_size=2000, weight=None, info=True).to(gpu)\n",
    "    \n",
    "    X_tr = [Xy[_] for _ in tr_ix]  # predict uses X and y as a tuple\n",
    "    X_ts = get_X([Xy[_] for _ in ts_ix])\n",
    "    y_ts = get_y([Xy[_] for _ in ts_ix])\n",
    "    \n",
    "    rnn.fit(X_tr)\n",
    "    y_pred = rnn.predict(X_ts)\n",
    "    \n",
    "    Acc += [np.sum(np.array(y_pred) == np.array(y_ts))/len(y_pred)]\n",
    "\n",
    "    cm_y += y_ts\n",
    "    cm_p += y_pred.tolist()\n",
    "\n",
    "print(f'RNN 10-fold CV Acc= {np.mean(Acc):.2f} {chr(177)}{np.std(Acc):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this did not cause a performance drop(I could not figure out why), one of the reasons that a performance drop would be expected is due to an overwhelmingly unbalanced dataset in favor of english over scottish. Due to this unbalance, the classifier would be overwhelmingly in favor of predicting English.txt over Scottish.txt. This is because the model may exhibit bias towards the English class, since statistically speaking, it is more likely to appear. When we add weights, it allows us to normalize the model, so that the model is unlikely to predict English anymore than Scottish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. [20 pts] Now set the performance metric to F1-score, repeat (3.) and (4.) report your observations. (If you had already used F1-score in (3.) just comment why accuracy was not the right metric for this problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.69 | Avg loss:   0.35 | 0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.69 | Avg loss:   0.35 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.69 | Avg loss:   0.35 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.69 | Avg loss:   0.35 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.69 | Avg loss:   0.35 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.69 | Avg loss:   0.35 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.69 | Avg loss:   0.35 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.69 | Avg loss:   0.35 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "001/1000 | Loss:   0.69 | Avg loss:   0.69 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.69 | Avg loss:   0.35 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000/1000 | Loss:   0.05 | Avg loss:   0.00 | 0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN 10-fold CV F1-Score = 0.77\n",
      "CPU times: user 27min 50s, sys: 28.7 s, total: 28min 18s\n",
      "Wall time: 14min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score\n",
    "Xy = get_Xy()\n",
    "\n",
    "cm_y, cm_p = [], []\n",
    "\n",
    "Acc = []\n",
    "kf = StratifiedKFold(n_splits=10)\n",
    "i=0\n",
    "for tr_ix, ts_ix in kf.split(np.arange(len(Xy)), get_y(Xy)):\n",
    "    i+=1\n",
    "    print(i)\n",
    "    rnn = My_RNN(128, n_hid_layers=2, epochs=1000, eta=0.005, batch_size=2000, weight=WEIGHTS, info=True).to(gpu)\n",
    "    \n",
    "    X_tr = [Xy[_] for _ in tr_ix]  # predict uses X and y as a tuple\n",
    "    X_ts = get_X([Xy[_] for _ in ts_ix])\n",
    "    y_ts = get_y([Xy[_] for _ in ts_ix])\n",
    "    \n",
    "    rnn.fit(X_tr)\n",
    "    y_pred = rnn.predict(X_ts)\n",
    "    \n",
    "    Acc += [np.sum(np.array(y_pred) == np.array(y_ts))/len(y_pred)]\n",
    "\n",
    "    cm_y += y_ts\n",
    "    cm_p += y_pred.tolist()\n",
    "f1 = f1_score(cm_y, cm_p, average='weighted')\n",
    "print(f'RNN 10-fold CV F1-Score = {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.70 | Avg loss:   0.35 | 0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.73 | Avg loss:   0.36 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.68 | Avg loss:   0.34 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.70 | Avg loss:   0.35 | 10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "001/1000 | Loss:   0.78 | Avg loss:   0.78 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.65 | Avg loss:   0.33 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.63 | Avg loss:   0.31 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.73 | Avg loss:   0.37 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.72 | Avg loss:   0.36 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "002/1000 | Loss:   0.62 | Avg loss:   0.31 | 00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000/1000 | Loss:   0.03 | Avg loss:   0.00 | 0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN 10-fold CV F1-Score = 0.85\n",
      "CPU times: user 27min 28s, sys: 21.7 s, total: 27min 50s\n",
      "Wall time: 13min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score\n",
    "Xy = get_Xy()\n",
    "\n",
    "cm_y, cm_p = [], []\n",
    "\n",
    "Acc = []\n",
    "kf = StratifiedKFold(n_splits=10)\n",
    "i=0\n",
    "for tr_ix, ts_ix in kf.split(np.arange(len(Xy)), get_y(Xy)):\n",
    "    i+=1\n",
    "    print(i)\n",
    "    rnn = My_RNN(128, n_hid_layers=2, epochs=1000, eta=0.005, batch_size=2000, weight=None, info=True).to(gpu)\n",
    "    \n",
    "    X_tr = [Xy[_] for _ in tr_ix]  # predict uses X and y as a tuple\n",
    "    X_ts = get_X([Xy[_] for _ in ts_ix])\n",
    "    y_ts = get_y([Xy[_] for _ in ts_ix])\n",
    "    \n",
    "    rnn.fit(X_tr)\n",
    "    y_pred = rnn.predict(X_ts)\n",
    "    \n",
    "    Acc += [np.sum(np.array(y_pred) == np.array(y_ts))/len(y_pred)]\n",
    "\n",
    "    cm_y += y_ts\n",
    "    cm_p += y_pred.tolist()\n",
    "f1 = f1_score(cm_y, cm_p, average='weighted')\n",
    "print(f'RNN 10-fold CV F1-Score = {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One reason why accuracy may not be the right metric for this problem is becuase when there is an unbalanced dataset where 97% is one class and 3% is the other class, a model may simply achieve a high accuracy score by predicting the majority class. The f-1 score is better because it ensures that the model is not simply producing an excess of false positives or excess of false negatives to achive a high accuracy score, and allows us to make sure that a model is balanced."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
